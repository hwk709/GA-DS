import pandas as pd
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, OneHotEncoder
from sklearn.metrics import mean_squared_error

# Sample dataset
data = pd.DataFrame({
    'year': [2020, 2021, 2022, 2023, 2020, 2021, 2022, 2023],
    'program_code': ['A', 'A', 'B', 'B', 'C', 'C', 'D', 'D'],
    'funding_level': [1000, 1100, 1200, 1300, 900, 950, 1050, 1100],
    'requirement': [2000, 2200, 2400, 2600, 1800, 1900, 2100, 2300]
})

# ----- Apply Encoding -----
# Label Encoding for 'program_code' (ordinal encoding if there's an inherent order)
label_encoder = LabelEncoder()
data['program_code_encoded'] = label_encoder.fit_transform(data['program_code'])

# One-Hot Encoding for 'program_code' (no inherent order, so we use one-hot encoding)
one_hot_encoder = OneHotEncoder(sparse=False)
encoded_columns = one_hot_encoder.fit_transform(data[['program_code']])
encoded_df = pd.DataFrame(encoded_columns, columns=[f'program_code_{int(i)}' for i in range(encoded_columns.shape[1])])

# Combine original data with one-hot encoded columns
data = pd.concat([data, encoded_df], axis=1)

# Drop the original 'program_code' as we've encoded it
data = data.drop(columns=['program_code'])

# ----- Modeling -----
# Define features (including the encoded variables) and target variable
X_L2 = data.drop(columns=['requirement'])  # Independent variables
y_L2 = data['requirement']  # Target variable

# Split the data into training and testing sets
X_train_L2, X_test_L2, y_train_L2, y_test_L2 = train_test_split(X_L2, y_L2, test_size=0.2, random_state=42)

# Fit the Random Forest Regressor
rf_L2 = RandomForestRegressor(n_estimators=100, random_state=42)
rf_L2.fit(X_train_L2, y_train_L2)

# Predictions
y_pred_L2 = rf_L2.predict(X_test_L2)

# Model evaluation
mse_L2 = mean_squared_error(y_test_L2, y_pred_L2)
print(f"L2 MSE: {mse_L2}")
